{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNElUzNjJoEcGE+EbxIBLu5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuromasadev/Home_Sales/blob/main/home_sales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X6_39vNjuOF",
        "outputId": "83c8a82d-2032-47fe-d4d6-3f37eebdd79b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=8f2dbf64247f5e51db0c9b3fe357be35b519a359e6c389b9f7d3813a0fc2c6b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "%pip install findspark\n",
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "GW48p9x6kIdp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ],
      "metadata": {
        "id": "CeysrEX-kO3U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "file_path = SparkFiles.get(\"home_sales_revised.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0XQnny_1kQ5S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create a temporary view of the DataFrame.\n",
        "\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# 2. Create a temporary view of the DataFrame.\n",
        "df.createOrReplaceTempView(\"home_sales\")"
      ],
      "metadata": {
        "id": "NDn0MWaNkW_h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
        "average_prices = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        YEAR(date) AS year,\n",
        "        AVG(price) AS avg_price\n",
        "    FROM\n",
        "        home_sales\n",
        "    WHERE\n",
        "        bedrooms = 4\n",
        "    GROUP BY\n",
        "        year\n",
        "    ORDER BY\n",
        "        year\n",
        "\"\"\")\n",
        "\n",
        "average_prices.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udtxCqUlkdXe",
        "outputId": "e0d551da-999c-4ceb-cae7-1a6b357743d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|year|         avg_price|\n",
            "+----+------------------+\n",
            "|2019| 300263.6955128205|\n",
            "|2020|298353.78003169573|\n",
            "|2021|   301819.44398864|\n",
            "|2022| 296363.8845050215|\n",
            "+----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
        "average_prices2 = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        date_built AS year_built,\n",
        "        AVG(price) AS avg_price\n",
        "    FROM\n",
        "        home_sales\n",
        "    WHERE\n",
        "        bedrooms = 3 AND bathrooms = 3\n",
        "    GROUP BY\n",
        "        year_built\n",
        "    ORDER BY\n",
        "        year_built\n",
        "\"\"\")\n",
        "\n",
        "average_prices2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_5tHtUQkhMT",
        "outputId": "d9c0d030-1a06-430a-f292-e6ca9ec72202"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|year_built|avg_price         |\n",
            "+----------+------------------+\n",
            "|2010      |292859.615942029  |\n",
            "|2011      |291117.46706586826|\n",
            "|2012      |293683.1872074883 |\n",
            "|2013      |295962.27145085804|\n",
            "|2014      |290852.2661870504 |\n",
            "|2015      |288770.2966101695 |\n",
            "|2016      |290555.073964497  |\n",
            "|2017      |292676.7887740029 |\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
        "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
        "\n",
        "average_prices3 = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        date_built AS year_built,\n",
        "        AVG(price) AS avg_price\n",
        "    FROM\n",
        "        home_sales\n",
        "    WHERE\n",
        "        bedrooms = 3 AND\n",
        "        bathrooms = 3 AND\n",
        "        floors = 2 AND\n",
        "        sqft_living >= 2000\n",
        "    GROUP BY\n",
        "        year_built\n",
        "    ORDER BY\n",
        "        year_built\n",
        "\"\"\")\n",
        "\n",
        "average_prices3.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMStol8ZkkJJ",
        "outputId": "354e8bfa-f21a-452f-b4f4-e51a5ccc5860"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|year_built|avg_price         |\n",
            "+----------+------------------+\n",
            "|2010      |285010.2215909091 |\n",
            "|2011      |276553.8128654971 |\n",
            "|2012      |307539.97402597405|\n",
            "|2013      |303676.79375      |\n",
            "|2014      |298264.7183908046 |\n",
            "|2015      |297609.9679144385 |\n",
            "|2016      |293965.1046511628 |\n",
            "|2017      |280317.57692307694|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
        "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "average_view_rating = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        AVG(view) AS avg_view_rating\n",
        "    FROM\n",
        "        home_sales\n",
        "    WHERE\n",
        "        price >= 350000\n",
        "\"\"\").collect()[0][\"avg_view_rating\"]\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Average View Rating: {average_view_rating:.2f}\")\n",
        "print(\"--- %s seconds ---\" % (end_time - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA5Ds01Tknoj",
        "outputId": "edd8824c-2a8e-45e8-83fa-640887b85422"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average View Rating: 32.26\n",
            "--- 0.7893130779266357 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.sql(\"CACHE TABLE home_sales\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1Z29Nbtkqn9",
        "outputId": "c4f7c14e-6310-43f8-8843-2607aa7e6cca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Check if the table is cached.\n",
        "spark.catalog.isCached('home_sales')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Ld6Q1FksB4",
        "outputId": "7cdf6a2e-3d49-4f1c-bc4c-8b78c04bf3b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Using the cached data, run the query that filters out the view ratings with average price\n",
        "#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "cached_results = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        AVG(view) AS avg_view_rating\n",
        "    FROM\n",
        "        home_sales\n",
        "    WHERE\n",
        "        price >= 350000\n",
        "\"\"\").collect()[0][\"avg_view_rating\"]\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "cached_runtime = end_time - start_time\n"
      ],
      "metadata": {
        "id": "SUx2doa0ktuj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "uncached_results = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        AVG(view) AS avg_view_rating\n",
        "    FROM\n",
        "        home_sales\n",
        "    WHERE\n",
        "        price >= 350000\n",
        "\"\"\").collect()[0][\"avg_view_rating\"]\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "uncached_runtime = end_time - start_time"
      ],
      "metadata": {
        "id": "QY4M3bZTkxdQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cached Runtime: {cached_runtime:.4f} seconds\")\n",
        "print(f\"Uncached Runtime: {uncached_runtime:.4f} seconds\")\n",
        "print(f\"Cached Results: {cached_results:.2f}\")\n",
        "print(f\"Uncached Results: {uncached_results:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHrCaQ3Okya7",
        "outputId": "f8d52819-a56d-4fdf-b67e-a84d218456f6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached Runtime: 0.7879 seconds\n",
            "Uncached Runtime: 0.4425 seconds\n",
            "Cached Results: 32.26\n",
            "Uncached Results: 32.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "\n",
        "output_path = \"partitioned_data/\"\n",
        "df.write.partitionBy(\"date_built\").parquet(output_path)"
      ],
      "metadata": {
        "id": "BCTiSJxUlGxb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Read the formatted Parquet data\n",
        "parquet_path = output_path\n",
        "parquet_df = spark.read.parquet(parquet_path)"
      ],
      "metadata": {
        "id": "05k3lM_dlUFd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm6wbzuXlkqe",
        "outputId": "30831c7f-62e6-499c-87d7-43bacba6598c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            " |-- bedrooms: integer (nullable = true)\n",
            " |-- bathrooms: integer (nullable = true)\n",
            " |-- sqft_living: integer (nullable = true)\n",
            " |-- sqft_lot: integer (nullable = true)\n",
            " |-- floors: integer (nullable = true)\n",
            " |-- waterfront: integer (nullable = true)\n",
            " |-- view: integer (nullable = true)\n",
            " |-- date_built: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Create a temporary table for the Parquet data\n",
        "parquet_df.createOrReplaceTempView(\"parquet_data\")"
      ],
      "metadata": {
        "id": "LSuRg3p6loAT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Run the query that filters out the view ratings with average price of greater than or eqaul to $350,000\n",
        "# with the parquet DataFrame. Round your average to two decimal places.\n",
        "# Determine the runtime and compare it to the cached version.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "parquet_results = parquet_df.filter(parquet_df[\"price\"] >= 350000) \\\n",
        "                           .groupBy().avg(\"view\") \\\n",
        "                           .collect()[0][0]\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "parquet_runtime = end_time - start_time\n",
        "\n",
        "print(f\"Parquet Query Runtime: {parquet_runtime:.4f} seconds\")\n",
        "print(f\"Parquet Results: {parquet_results:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv8iZLNHl93S",
        "outputId": "a098778e-3bdb-4564-f593-04d084936d71"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet Query Runtime: 1.0986 seconds\n",
            "Parquet Results: 32.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pcached_start_time = time.time()\n",
        "\n",
        "pcached_results = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        AVG(view) AS avg_view_rating\n",
        "    FROM\n",
        "        home_sales\n",
        "    WHERE\n",
        "        price >= 350000\n",
        "\"\"\").collect()[0][\"avg_view_rating\"]\n",
        "\n",
        "pcached_end_time = time.time()\n",
        "\n",
        "pcached_runtime = pcached_end_time - pcached_start_time\n",
        "\n",
        "print(f\"Cached Query Runtime: {pcached_runtime:.4f} seconds\")\n",
        "print(f\"Cached Results: {pcached_results:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MnURVcDl_He",
        "outputId": "03f75c95-33e1-49b1-95e2-e22bf36dc36f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached Query Runtime: 0.3760 seconds\n",
            "Cached Results: 32.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.catalog.uncacheTable(\"home_sales\")\n"
      ],
      "metadata": {
        "id": "RHPs7Ud9mZfn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "\n",
        "is_cached = spark.catalog.isCached(\"home_sales\")\n",
        "\n",
        "if is_cached:\n",
        "    print(\"The 'home_sales' temporary table is still cached.\")\n",
        "else:\n",
        "    print(\"The 'home_sales' temporary table is not cached.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I0EIKlbmZ95",
        "outputId": "f5d1e86e-f1c9-4d03-972a-8176b756a31f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'home_sales' temporary table is not cached.\n"
          ]
        }
      ]
    }
  ]
}